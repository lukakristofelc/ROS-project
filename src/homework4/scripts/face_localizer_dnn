#!/usr/bin/python3

import sys
import rospy
import dlib
import cv2
import numpy as np
import tf2_geometry_msgs
import tf2_ros
#import matplotlib.pyplot as plt
from sensor_msgs.msg import Image
from geometry_msgs.msg import PointStamped, Vector3, Pose
from cv_bridge import CvBridge, CvBridgeError
from visualization_msgs.msg import Marker, MarkerArray
from std_msgs.msg import ColorRGBA
from scipy.spatial import distance
from nav_msgs.msg import Odometry
from homework4.msg import FaceGoals,FaceGoalsArray


class face_localizer:
    def __init__(self):
        rospy.init_node('face_localizer', anonymous=True)

        # An object we use for converting images between ROS format and OpenCV format
        self.bridge = CvBridge()

        # The function for performin HOG face detection
        #self.face_detector = dlib.get_frontal_face_detector()
        self.face_net = cv2.dnn.readNetFromCaffe('/home/krblaz/Documents/RIS/src/exercise4/scripts/deploy.prototxt.txt', '/home/krblaz/Documents/RIS/src/exercise4/scripts/res10_300x300_ssd_iter_140000.caffemodel')

        # A help variable for holding the dimensions of the image
        self.dims = (0, 0, 0)

        # Subscribe to the image and/or depth topic
        # self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback)
        # self.depth_sub = rospy.Subscriber("/camera/depth/image_raw", Image, self.depth_callback)

        # Publiser for the visualization markers
        self.markers_pub = rospy.Publisher('face_markers', MarkerArray, queue_size=1000)
        self.face_goals_pub = rospy.Publisher('face_goals', FaceGoalsArray, queue_size=1000)

        # Object we use for transforming between coordinate frames
        self.tf_buf = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buf)

        self.num = 0
        self.centres = np.empty((10,3))
        self.all_points = []
        self.centres_offsets = np.empty((10,3))
        self.all_points_offsets = []

    def get_pose_new(self,coords,depth_image,stamp):
        k_f = 554

        x1, x2, y1, y2 = coords

        face_x1 = self.dims[1] / 2 - x1
        face_x2 = self.dims[1] / 2 - x2

        angle_to_target1 = np.arctan2(face_x1,k_f)
        angle_to_target2 = np.arctan2(face_x2,k_f)
        
        try:
            dist1 = float(np.nanmean(depth_image[y1:y2,x1]))
            dist2 = float(np.nanmean(depth_image[y1:y2,x2]))
        except Exception as e:
            print(e)
            return None,None

        x1, y1 = dist1*np.cos(angle_to_target1), dist1*np.sin(angle_to_target1)
        x2, y2 = dist2*np.cos(angle_to_target2), dist2*np.sin(angle_to_target2)

        point_s1 = PointStamped()
        point_s1.point.x = -y1
        point_s1.point.y = 0
        point_s1.point.z = x1
        point_s1.header.frame_id = "camera_rgb_optical_frame"
        point_s1.header.stamp = stamp

        point_s2 = PointStamped()
        point_s2.point.x = -y2
        point_s2.point.y = 0
        point_s2.point.z = x2
        point_s2.header.frame_id = "camera_rgb_optical_frame"
        point_s2.header.stamp = stamp

        try:
            point_world1 = self.tf_buf.transform(point_s1, "map")
            point_world2 = self.tf_buf.transform(point_s2, "map")
            pose1 = np.array([point_world1.point.x,point_world1.point.y,point_world1.point.z])
            pose2 = np.array([point_world2.point.x,point_world2.point.y,point_world2.point.z])
        except Exception as e:
            print(e)
            pose1 = None
            pose2 = None
        return pose1,pose2




    def get_pose(self,coords,dist,stamp):
        # Calculate the position of the detected face

        k_f = 554 # kinect focal length in pixels

        x1, x2, y1, y2 = coords

        face_x = self.dims[1] / 2 - (x1+x2)/2.
        face_y = self.dims[0] / 2 - (y1+y2)/2.

        angle_to_target = np.arctan2(face_x,k_f)
        

        # Get the angles in the base_link relative coordinate system
        x, y = dist*np.cos(angle_to_target), dist*np.sin(angle_to_target)

        ### Define a stamped message for transformation - directly in "base_link"
        #point_s = PointStamped()
        #point_s.point.x = x
        #point_s.point.y = y
        #point_s.point.z = 0.3
        #point_s.header.frame_id = "base_link"
        #point_s.header.stamp = rospy.Time(0)

        # Define a stamped message for transformation - in the "camera rgb frame"
        point_s = PointStamped()
        point_s.point.x = -y
        point_s.point.y = 0
        point_s.point.z = x
        point_s.header.frame_id = "camera_rgb_optical_frame"
        point_s.header.stamp = stamp

        # Get the point in the "map" coordinate system
        try:
            point_world = self.tf_buf.transform(point_s, "map")

            pose = np.array([point_world.point.x,point_world.point.y,point_world.point.z])
        except Exception as e:
            print(e)
            pose = None

        return pose
    

    def find_faces(self):
        #print('I got a new image!')

        # Get the next rgb and depth images that are posted from the camera
        try:
            rgb_image_message = rospy.wait_for_message("/camera/rgb/image_raw", Image)
        except Exception as e:
            print(e)
            return 0

        try:
            depth_image_message = rospy.wait_for_message("/camera/depth/image_raw", Image)
        except Exception as e:
            print(e)
            return 0

        try:
            odom:Odometry = rospy.wait_for_message("/odom",Odometry)
        except Exception as e:
            print(e)
            return 0

        # Convert the images into a OpenCV (numpy) format

        try:
            rgb_image = self.bridge.imgmsg_to_cv2(rgb_image_message, "bgr8")
        except CvBridgeError as e:
            print(e)

        try:
            depth_image = self.bridge.imgmsg_to_cv2(depth_image_message, "32FC1")
        except CvBridgeError as e:
            print(e)

        # Set the dimensions of the image
        self.dims = rgb_image.shape
        h = self.dims[0]
        w = self.dims[1]

        # Tranform image to gayscale
        #gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)

        # Do histogram equlization
        #img = cv2.equalizeHist(gray)

        # Detect the faces in the image
        #face_rectangles = self.face_detector(rgb_image, 0)
        blob = cv2.dnn.blobFromImage(cv2.resize(rgb_image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        self.face_net.setInput(blob)
        face_detections = self.face_net.forward()

        for i in range(0, face_detections.shape[2]):
            confidence = face_detections[0, 0, i, 2]
            if confidence>0.5:
                box = face_detections[0,0,i,3:7] * np.array([w,h,w,h])
                box = box.astype('int')
                x1, y1, x2, y2 = box[0], box[1], box[2], box[3]

                # Extract region containing face
                face_region = rgb_image[y1:y2, x1:x2]

                # Visualize the extracted face
                #cv2.imshow("ImWindow", face_region)
                #cv2.waitKey(1)

                # Find the distance to the detected face
                face_distance = float(np.nanmean(depth_image[y1:y2,x1:x2]))

                #print('Distance to face', face_distance)

                # Get the time that the depth image was recieved
                depth_time = depth_image_message.header.stamp

                # Find the location of the detected face
                #point = self.get_pose((x1,x2,y1,y2), face_distance, depth_time)

                point1,point2 = self.get_pose_new((x1,x2,y1,y2),depth_image,depth_time)

                if point1 is not None and point2 is not None:
                    point = (point1+point2)/2
                    vec = point1-point2
                    offset = np.array([1,-vec[0]/vec[1],0])
                    offset1 = offset / np.linalg.norm(offset) * 0.4
                    offset2 = -offset1
                    position = np.array([odom.pose.pose.position.x,odom.pose.pose.position.y,0])
                    offset = offset1 if np.linalg.norm(point+offset1-position) <= np.linalg.norm(point+offset2-position) else offset2

                    d = distance.cdist([point],self.centres[:self.num,:])
                    if self.num==0 or np.min(d)>0.5:
                        self.all_points.append([])
                        self.all_points[self.num].append(point)
                        self.all_points_offsets.append([])
                        self.all_points_offsets[self.num].append(offset)
                        self.centres[self.num,:]=point
                        self.centres_offsets[self.num,:]=offset
                        self.num+=1
                    else:
                        i = np.argmin(d)
                        self.all_points[i].append(point)
                        tp = np.array(self.all_points[i])
                        self.centres[i,:]=np.mean(tp,axis=0)
                        self.all_points_offsets[i].append(offset)
                        tg = np.array(self.all_points_offsets[i])
                        self.centres_offsets[i,:]=np.mean(tg,axis=0)

        # Create a marker used for visualization
        marker_array = MarkerArray()
        face_goal_array = FaceGoalsArray()
        for i in range(self.num):
            marker = Marker()
            marker.header.stamp = rospy.Time(0)
            marker.header.frame_id = 'map'

            pose = Pose()
            pose.position.x = self.centres[i,0]
            pose.position.y = self.centres[i,1]
            pose.position.z = self.centres[i,2]

            marker.pose = pose
            marker.type = Marker.TEXT_VIEW_FACING
            marker.text = "{}".format(len(self.all_points[i]))
            marker.action = Marker.ADD
            marker.frame_locked = False
            marker.lifetime = rospy.Duration.from_sec(10)
            marker.id = i
            marker.scale = Vector3(0.5, 0.5, 0.5)
            marker.color = ColorRGBA(1, 0, 0, 1)
            marker_array.markers.append(marker)

        for i in range(self.num):
            marker = Marker()
            marker.header.stamp = rospy.Time(0)
            marker.header.frame_id = 'map'

            pose = Pose()
            pose.position.x = self.centres[i,0] + self.centres_offsets[i,0]
            pose.position.y = self.centres[i,1] + self.centres_offsets[i,1]
            pose.position.z = self.centres[i,2] + self.centres_offsets[i,2]

            marker.pose = pose
            marker.type = Marker.CUBE
            marker.action = Marker.ADD
            marker.frame_locked = False
            marker.lifetime = rospy.Duration.from_sec(10)
            marker.id = self.num+i
            marker.scale = Vector3(0.1, 0.1, 0.1)
            marker.color = ColorRGBA(1, 0, 1, 1)
            marker_array.markers.append(marker)

            face_goal = FaceGoals()
            face_goal.coords=(self.centres[i,:2]+self.centres_offsets[i,:2]).tolist()
            angle=np.arctan2(-self.centres_offsets[i,1],-self.centres_offsets[i,0])
            face_goal.coords.extend([np.sin(angle/2),np.cos(angle/2)])
            face_goal_array.goals.append(face_goal)

        self.markers_pub.publish(marker_array)
        self.face_goals_pub.publish(face_goal_array)

    def depth_callback(self,data):

        try:
            depth_image = self.bridge.imgmsg_to_cv2(data, "32FC1")
        except CvBridgeError as e:
            print(e)

        # Do the necessairy conversion so we can visuzalize it in OpenCV
        
        image_1 = depth_image / np.nanmax(depth_image)
        image_1 = image_1*255
        
        image_viz = np.array(image_1, dtype=np.uint8)

        #cv2.imshow("Depth window", image_viz)
        #cv2.waitKey(1)

        #plt.imshow(depth_image)
        #plt.show()

def main():

        face_finder = face_localizer()

        rate = rospy.Rate(1)
        while not rospy.is_shutdown():
            face_finder.find_faces()
            rate.sleep()

        cv2.destroyAllWindows()


if __name__ == '__main__':
    main()
